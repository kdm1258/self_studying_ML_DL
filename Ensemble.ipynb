{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c280d5c1-9407-4fd4-b311-574c840b2102",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "앙상블 기법은 **여러 개의 모델을 결합**하여 더 나은 예측 성능을 얻는 머신러닝 기법이다. 이 방법은 각 모델의 단점을 보완하고, 결과의 안정성을 높이는 데 유용하다. \n",
    "\n",
    "1. 배깅(Bagging):\n",
    "\n",
    "    여러 개의 학습 데이터를 랜덤하게 선택하여 각각의 모델을 학습시킨 후, 결과를 평균내거나 다수결 투표로 결합합니다.\n",
    "\n",
    "    예: 랜덤 포레스트(Random Forest)\n",
    "\n",
    "2. 부스팅(Boosting):\n",
    "\n",
    "    약한 학습기(weak learner)를 순차적으로 학습시키며, 이전 모델의 오차를 줄이는 방향으로 새로운 모델을 추가합니다.\n",
    "\n",
    "    예: 에이다부스트(AdaBoost), 그라디언트 부스팅 머신(GBM), XGBoost\n",
    "\n",
    "3. 스태킹(Stacking):\n",
    "\n",
    "    여러 개의 학습 모델을 결합하여 새로운 메타 모델(meta-model)을 학습시킵니다.\n",
    "\n",
    "    각 모델의 예측 결과를 메타 모델의 입력으로 사용합니다.\n",
    "\n",
    "4. 보팅(Voting):\n",
    "\n",
    "    여러 개의 모델을 각각 학습시키고, 결과를 다수결 투표 방식으로 결합합니다.\n",
    "\n",
    "    예: 소프트 보팅(Soft Voting), 하드 보팅(Hard Voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a35257-432d-4da0-b356-2b6c79107225",
   "metadata": {},
   "source": [
    "## 랜덤 포레스트(Random Forest)\n",
    "랜덤 포레스트(Random Forest)는 배깅(Bagging) 기법을 사용한 앙상블 학습 방법의 하나로, 여러 개의 결정 트리(Decision Tree)를 결합하여 예측 성능을 향상시키는 알고리즘이다. \n",
    "\n",
    "- 동작 과정:\n",
    "    1. 데이터 샘플링:\n",
    "    \n",
    "        원본 학습 데이터를 여러 개의 부분 집합으로 랜덤하게 샘플링한다. 이때 각 샘플은 원본 데이터와 동일한 크기를 가지며, 중복을 허용하는 방식으로 샘플링된다(부트스트래핑).\n",
    "\n",
    "    2. 모델 학습:\n",
    "\n",
    "        각 부분 집합마다 하나의 결정 트리를 학습시킴. 이때 트리의 각 노드에서 분할할 때, 전체 특징(feature) 중 일부를 랜덤하게 선택하여 최적의 분할을 찾음. 이를 통해 트리의 다양성을 높인다.\n",
    "\n",
    "    3. 예측 결합:\n",
    "\n",
    "        학습된 여러 개의 결정 트리로 새로운 데이터를 예측할 때, 각 트리의 예측 결과를 결합하여 최종 결과를 도출. 분류 문제에서는 **다수결 투표** 방식으로, 회귀 문제에서는 평균을 내는 방식으로 결합.\n",
    "\n",
    "- 랜덤 포레스트의 주요 장점:\n",
    "\n",
    "    1. 높은 예측 성능: 여러 개의 모델을 결합하므로 개별 결정 트리보다 일반화 성능이 뛰어나다.\n",
    "\n",
    "    2. 과적합 방지: 트리의 다양성과 모델의 결합을 통해 과적합(overfitting)을 방지한다. 랜덤하게 선택한 샘플과 특성(RandomForestClassifier는 root N개의 특성, RandomForestRegressor은 전체 특성 사용)사용.\n",
    "\n",
    "    3. 특성 중요도 측정: 각 특성의 중요도를 평가할 수 있어 해석 가능성이 높다.\n",
    "\n",
    "하지만 랜덤 포레스트는 많은 트리를 생성하므로 계산 비용이 높음. 따라서 대규모 데이터셋에서는 계산 효율성을 고려해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ad728-3123-403b-abe4-ea63d8957085",
   "metadata": {},
   "source": [
    "## 부트스트랩(Bootstrap)\n",
    "부트스트랩은 통계학 및 머신러닝에서 데이터의 신뢰성을 평가하기 위해 사용되는 기법이다. 원본 데이터를 여러 번 샘플링하여 새로운 데이터셋을 만들고, 이를 통해 통계적 추정치를 계산하는 과정을 거친다.\n",
    "\n",
    "- 부트스트랩의 주요 단계\n",
    "    1. 데이터 샘플링:\n",
    "\n",
    "    원본 데이터에서 **중복을 허용하여(복원추출)** 랜덤하게 샘플을 추출한다. 각 샘플링 과정에서 원본 데이터의 크기와 동일한 크기의 새로운 데이터셋을 만든다.\n",
    "\n",
    "    2. 모델 학습 및 평가:\n",
    "\n",
    "    각 샘플링된 데이터셋을 사용하여 모델을 학습시키고, 성능을 평가한다. 이 과정을 여러 번 반복하여 결과를 수집한다.\n",
    "\n",
    "    3. 결과 분석:\n",
    "\n",
    "    여러 번의 반복을 통해 얻어진 성능 평가 결과를 분석하여 최종적인 통계적 추정치를 계산한다. 이를 통해 모델의 신뢰성과 안정성을 평가할 수 있다.\n",
    "\n",
    "- 장점\n",
    "    1. 신뢰성 향상: 부트스트랩을 통해 모델의 성능 평가가 더욱 신뢰성 있게 이루어진다.\n",
    "\n",
    "    2. 데이터 활용 극대화: 한정된 데이터로도 다양한 샘플을 생성하여 분석할 수 있다.\n",
    "\n",
    "부트스트랩 방법은 머신러닝 모델의 성능 평가뿐만 아니라, 통계적 가설 검정 및 변수 중요도 평가 등 다양한 분야에서 활용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df2cd425-f4be-40aa-8aa4-2660d7d66b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('http://bit.ly/wine_csv_data')\n",
    "data = wine[['alcohol','sugar','pH']].to_numpy()\n",
    "target = wine['class'].to_numpy()\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d69bb045-e095-4aa5-8c6a-e1b52ea3223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973541965122431 0.8905151032797809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs= -1, random_state=42)\n",
    "scores = cross_validate(rf, train_input, train_target, return_train_score= True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c982fbb-c1c8-4c02-b55f-afde514b5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23167441 0.50039841 0.26792718]\n"
     ]
    }
   ],
   "source": [
    "rf.fit(train_input, train_target)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c0ff585-5b38-48e7-8706-850e913db034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8934000384837406\n"
     ]
    }
   ],
   "source": [
    "# 부스트 트랩 샘플에 포함되지 않고 남는 샘플을 이용해 부스트트랩샘플로 훈련한 결정트리를 평가한다.\n",
    "# oob(out of bag)_score = True로 설정\n",
    "rf = RandomForestClassifier(oob_score=True, n_jobs= -1, random_state=42)\n",
    "rf.fit(train_input, train_target)\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389db738-bcb6-4e89-acf7-6af91b60e6f2",
   "metadata": {},
   "source": [
    "## 엑스트라 트리(Extremely Randomized Trees)\n",
    "엑스트라 트리, 또는 엑스트림 랜덤 트리(Extremely Randomized Trees)는 랜덤 포레스트와 유사한 앙상블 학습 방법 중 하나로, 여러 개의 결정 트리를 결합하여 예측 성능을 높이는 기법이다. 엑스트라 트리는 트리의 다양성을 극대화하여 과적합을 방지하고, 예측 성능을 향상시키는 것을 목표로 한다.\n",
    "\n",
    "- 엑스트라 트리의 주요 특징\n",
    "    1. 완전한 무작위성:\n",
    "\n",
    "        각 트리의 각 분할 시점에서 무작위로 선택된 특성을 사용하여 분할을 수행한다. 이로 인해 트리의 다양성이 극대화된다.\n",
    "    \n",
    "    2. 부트스트랩 샘플링 없음:\n",
    "\n",
    "        원본 데이터에서 부트스트랩 샘플링을 사용하지 않고, **전체 데이터셋**을 사용하여 각 트리를 학습시킨다. 이는 **랜덤 포레스트와의 주요 차이점** 중 하나이다.\n",
    "\n",
    "    3. 결정 경계의 다양성:\n",
    "\n",
    "        각 트리의 **분할 기준을 무작위**로 선택하므로, 트리의 결정 경계가 매우 다양해진다. 이는 모델의 안정성을 높이는 데 기여한다.\n",
    "\n",
    "- 장점\n",
    "    1. 높은 예측 성능: 트리의 다양성을 극대화함으로써 예측 성능이 향상된다.\n",
    "\n",
    "    2. 과적합 방지: 트리의 무작위성을 통해 과적합을 방지할 수 있다.\n",
    "\n",
    "    3. 빠른 학습 속도: 부트스트랩 샘플링을 사용하지 않으므로 학습 속도가 빠르다.\n",
    "\n",
    "엑스트라 트리는 일반적으로 랜덤 포레스트와 함께 사용되며, 두 방법의 장점을 결합하여 더 높은 예측 성능을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5098e668-edf3-4c08-91ea-68873a50a05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974503966084433 0.8887848893166506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et = ExtraTreesClassifier(n_jobs= -1, random_state=42)\n",
    "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5861be3-1725-479a-9c57-45eb46c1088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20183568 0.52242907 0.27573525]\n"
     ]
    }
   ],
   "source": [
    "et.fit(train_input, train_target)\n",
    "print(et.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc168793-2ce9-4e2e-bd14-7b520cdfb138",
   "metadata": {},
   "source": [
    "## 그래디언트 부스팅(Gradient Boosting)\n",
    "그래디언트 부스팅은 여러 개의 약한 학습기(weak learner)를 결합하여 강한 예측 모델을 만드는 앙상블 기법이다. 주로 결정 트리(decision tree)를 약한 학습기로 사용하며, 이전 모델의 오차를 줄이는 방향으로 새로운 모델을 추가하는 방식으로 동작한다. 그래디언트 부스팅의 주요 단계는 다음과 같다.\n",
    "\n",
    "- 주요 단계\n",
    "    1. 초기 모델 학습:\n",
    "\n",
    "        첫 번째 약한 학습기를 사용하여 초기 모델을 학습시킨다. 이 모델은 보통 단순한 결정 트리이다.\n",
    "\n",
    "    2. 잔여 오차 계산:\n",
    "\n",
    "        초기 모델의 예측값과 실제값 사이의 오차(잔여)를 계산한다.\n",
    "\n",
    "    3. 잔여 오차를 예측하는 모델 학습:\n",
    "\n",
    "        두 번째 약한 학습기를 사용하여 앞서 계산한 잔여 오차를 예측하는 모델을 학습시킨다. 이 모델은 첫 번째 모델의 예측을 보완하는 역할을 한다.\n",
    "\n",
    "    4. 모델 업데이트:\n",
    "\n",
    "        새로운 모델을 기존 모델에 더하여 업데이트한다. 이를 통해 전체 모델의 예측 성능을 향상시킨다.\n",
    "\n",
    "    5. 반복:\n",
    "\n",
    "        위 과정을 여러 번 반복하여 모델을 계속해서 개선한다. 각 반복 단계에서는 이전 모델의 오차를 줄이는 방향으로 새로운 모델을 추가한다.\n",
    "\n",
    "- 장점\n",
    "    높은 예측 성능: 여러 개의 약한 학습기를 결합하여 강한 예측 모델을 만들기 때문에 높은 예측 성능을 가진다.\n",
    "\n",
    "    유연성: 분류, 회귀 등 다양한 문제에 적용할 수 있다.\n",
    "\n",
    "    과적합 방지: 적절한 정규화 기법을 사용하면 과적합을 방지할 수 있다.\n",
    "\n",
    "- 단점\n",
    "    학습 시간: 많은 반복 과정을 거치기 때문에 학습 시간이 길어질 수 있다.\n",
    "\n",
    "    복잡성: 모델이 복잡해지면서 해석이 어려워질 수 있다.\n",
    "\n",
    "그래디언트 부스팅은 앙상블 기법 중에서도 매우 강력한 예측 성능을 보여주며, 실제로 많은 경진대회와 실무에서 널리 사용되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f84f4e65-ca1a-4e26-92ae-db9902aaeade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881086892152563 0.8720430147331015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(gb, train_input, train_target, return_train_score= True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "#GradientBoosting은 과대적합에 강하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f3fb7f3-ff29-4936-9c7d-ee8dae29fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9464595437171814 0.8780082549788999\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
    "scores = cross_validate(gb, train_input, train_target, return_train_score= True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27493289-acba-486a-b5bd-fe2ac558adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15882696 0.6799705  0.16120254]\n"
     ]
    }
   ],
   "source": [
    "gb.fit(train_input, train_target)\n",
    "print(gb.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefda57c-0e03-4123-a141-ed44285e833c",
   "metadata": {},
   "source": [
    "## 히스토그램 기반 그레디언트 부스팅\n",
    "\n",
    "히스토그램 기반 그래디언트 부스팅(Histogram-based Gradient Boosting)은 입력 특성을 여러 구간으로 나누어 데이터를 히스토그램으로 변환하여 학습 및 예측에 활용하는 방식이다. 이 방법은 특히 범주형 변수를 처리하는 데 유리하며, 빠르고 효율적인 학습을 가능하게 한다.\n",
    "\n",
    "주요 특징\n",
    "\n",
    "    - 입력 특성 구간화: 입력 특성을 256개의 구간으로 나누고, 이를 통해 노드를 분할할 때 최적의 분할을 빠르게 찾는다.\n",
    "\n",
    "    - 누락된 값 처리: 256개의 구간 중 하나를 떼어 놓고, 누락된 값을 위해 사용함. 이를 통해 누락된 값 전처리 없이도 학습이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c5aa72c-7b91-4db6-8169-9cbeaa1af34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321723946453317 0.8801241948619236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(hgb, train_input, train_target, return_train_score=True)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42cfd913-e78a-487c-a40d-66aac188f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08876275 0.23438522 0.08027708]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "hgb.fit(train_input, train_target)\n",
    "result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state= 42, n_jobs=-1)\n",
    "print(result.importances_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ae668-36fb-4dba-9b5d-251c3d434ae1",
   "metadata": {},
   "source": [
    "permutation_importance() 함수가 변화하는 개겣는 반복하여 얻은 특성 중요도(importances), 평균(importances_mean), 표준편차(importances_std)를 담고있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7309a800-c803-42dc-9d3b-67c391fd5c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05969231 0.20238462 0.049     ]\n"
     ]
    }
   ],
   "source": [
    "result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "print(result.importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "31c662d7-0ab5-4e56-8885-4cb0e4270a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8723076923076923"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgb.score(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ac31369-43b3-4c40-90d1-972469065a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Collecting xgboost\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Collecting xgboost\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: numpy in c:\\users\\kdm12\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: numpy in c:\\users\\kdm12\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: scipy in c:\\users\\kdm12\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Requirement already satisfied: scipy in c:\\users\\kdm12\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fc632c8f204fce888a679d81cfda9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing collected packages: xgboost\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Installing collected packages: xgboost\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Successfully installed xgboost-2.1.4\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Successfully installed xgboost-2.1.4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 히스토그램 기반 그레디언트 부스팅 **회귀**\n",
    "# XGBoost\n",
    "import pip\n",
    "pip.main(['install', 'xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7716106f-3d70-4cd9-b510-312b00135750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9558403027491312 0.8782000074035686\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
    "scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9f2604a-e8a8-41cd-8cda-6331eb7d0b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935828414851749 0.8801251203079884\n"
     ]
    }
   ],
   "source": [
    "# 히스토그램 기반 그레디언트 부스팅 **회귀**\n",
    "# LightGBM\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "scores = cross_validate(lgb, train_input, train_target, return_train_score= True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973ab13-10b8-4b07-8028-18f2ee7fbb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
